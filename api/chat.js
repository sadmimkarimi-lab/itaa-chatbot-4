// api/chat.js

// â€”â€”â€”â€”â€”â€”â€” ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø®Ø±ÙˆØ¬ÛŒ â€”â€”â€”â€”â€”â€”â€”
function cleanAnswer(text) {
  if (!text || typeof text !== "string") return "Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ù¾Ø§Ø³Ø®ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù….";

  let t = text.trim();
  t = t.replace(/\r\n/g, "\n");
  t = t.replace(/\n{3,}/g, "\n\n");

  const lines = t.split("\n").map((line) => line.replace(/\s+$/g, ""));
  return lines.join("\n");
}

export default async function handler(req, res) {
  // ÙÙ‚Ø· POST Ù‚Ø¨ÙˆÙ„ Ú©Ù†ÛŒÙ…
  if (req.method !== "POST") {
    return res.status(405).json({ ok: false, error: "ÙÙ‚Ø· Ù…ØªØ¯ POST Ù…Ø¬Ø§Ø² Ø§Ø³Øª." });
  }

  const { text } = req.body || {};

  if (!text || typeof text !== "string") {
    return res
      .status(400)
      .json({ ok: false, error: "Ù…ØªÙ† Ø³Ø¤Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª." });
  }

  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) {
    console.error("GROQ_API_KEY ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.");
    return res
      .status(500)
      .json({ ok: false, error: "Ú©Ù„ÛŒØ¯ Ú¯Ø±ÙˆÚ© Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª." });
  }

  // â€”â€”â€”â€”â€”â€”â€” ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØµØ¯Ø§ Ø²Ø¯Ù† ÛŒÚ© Ù…Ø¯Ù„ â€”â€”â€”â€”â€”â€”â€”
  async function callGroqModel(modelName) {
    console.log("ğŸ” ØªØ³Øª Ù…Ø¯Ù„:", modelName);

    const response = await fetch(
      "https://api.groq.com/openai/v1/chat/completions",
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
          model: modelName,
          messages: [
            {
              role: "system",
              content:
                "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± ÙØ§Ø±Ø³ÛŒâ€ŒØ²Ø¨Ø§Ù†ØŒ ØµØ¨ÙˆØ± Ùˆ Ù…Ù‡Ø±Ø¨Ø§Ù† Ù‡Ø³ØªÛŒ. Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©ÙˆØªØ§Ù‡ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ù†ÙˆÛŒØ³.",
            },
            {
              role: "user",
              content: text,
            },
          ],
          temperature: 0.7,
        }),
      }
    );

    const data = await response.json().catch(() => ({}));

    if (!response.ok) {
      console.error("Groq error for model", modelName, data);
      const msg =
        data?.error?.message || `Groq error with model ${modelName}`;
      throw new Error(msg);
    }

    const answer = data?.choices?.[0]?.message?.content;
    if (!answer) {
      throw new Error("Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.");
    }

    return answer;
  }

  // â€”â€”â€”â€”â€”â€”â€” Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§: Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… â€”â€”â€”â€”â€”â€”â€”
  const modelsToTry = [
    "llama-3.1-8b-instant",  // Ø³Ø±ÛŒØ¹ Ùˆ Ø¹Ù…ÙˆÙ…ÛŒ
    "mixtral-8x7b-32768",    // Ù‚ÙˆÛŒâ€ŒØªØ±ØŒ Ø§Ú¯Ø± Ø§ÙˆÙ„ÛŒ Ø®Ø·Ø§ Ø¯Ø§Ø¯
    "qwen-2.5-coder-32b"     // fallback Ø³ÙˆÙ…
  ];

  try {
    let rawAnswer = null;
    let lastError = null;

    for (const model of modelsToTry) {
      try {
        rawAnswer = await callGroqModel(model);
        console.log("âœ… Ù…Ø¯Ù„ Ù…ÙˆÙÙ‚:", model);
        break; // ÙˆÙ‚ØªÛŒ ÛŒÚ© Ù…Ø¯Ù„ Ø¬ÙˆØ§Ø¨ Ø¯Ø§Ø¯ØŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø®Ø§Ø±Ø¬ Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…
      } catch (err) {
        lastError = err;
        console.error(`âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø¯Ù„ ${model}:`, err.message);
        // Ù…ÛŒâ€ŒØ±ÛŒÙ… Ø³Ø±Ø§Øº Ù…Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒ
      }
    }

    if (!rawAnswer) {
      // Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯Ù‡
      throw lastError || new Error("Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ù†ØªÙˆØ§Ù†Ø³Øª Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡Ø¯.");
    }

    const answer = cleanAnswer(rawAnswer);

    return res.status(200).json({ ok: true, answer });
  } catch (err) {
    console.error("Internal error:", err);
    return res.status(500).json({
      ok: false,
      error: "Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø³Ø±ÙˆØ±. Ú©Ù…ÛŒ Ø¨Ø¹Ø¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†.",
    });
  }
}
